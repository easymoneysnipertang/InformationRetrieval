{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 聚类网页内容，用于推荐系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取mysql数据库，获取网页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "cnx = pymysql.connect(host='localhost', user='root', password='123qwe12')\n",
    "cursor = cnx.cursor()\n",
    "cnx.select_db('IR_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_table(table_name):\n",
    "    cursor.execute(f\"SELECT id, title, content FROM {table_name}\")\n",
    "    results = cursor.fetchall()\n",
    "    data = []\n",
    "    for row in results:\n",
    "        data.append({\n",
    "            'id': row[0],\n",
    "            'title': row[1],\n",
    "            'content': row[2],\n",
    "            'type': 'douban'\n",
    "        })\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_ids_from_same_title():\n",
    "    # 创建SQL查询\n",
    "    sql = \"SELECT page_id FROM same_title\"\n",
    "    # 执行查询\n",
    "    cursor.execute(sql)\n",
    "    # 获取所有的结果\n",
    "    results = cursor.fetchall()\n",
    "    # 将结果从元组列表转换为普通列表\n",
    "    ids_same_title = [result[0] for result in results]\n",
    "    return ids_same_title\n",
    "\n",
    "\n",
    "# 有问题\n",
    "def get_page():\n",
    "    cursor.execute(\"SELECT id, title, content FROM page\")\n",
    "    results = cursor.fetchall()\n",
    "    # 获取相同title的id\n",
    "    ids_same_title = get_ids_from_same_title()\n",
    "    data = []\n",
    "    for row in results:\n",
    "        data.append({\n",
    "            'id': row[0],\n",
    "            'title': row[1],\n",
    "            'content': row[2],\n",
    "            'type': 'page'\n",
    "        })\n",
    "    # 从same_title中按title分组，获取每组的第一个id\n",
    "    sql = \"SELECT MIN(page_id) FROM same_title GROUP BY title\"\n",
    "    cursor.execute(sql)\n",
    "    results = cursor.fetchall()\n",
    "    ids = [result[0] for result in results]\n",
    "    print(len(ids))\n",
    "    # 找到这些id对应的url\n",
    "    for id in ids:\n",
    "        sql = \"SELECT id, title, content FROM page WHERE id = %d\" % id\n",
    "        cursor.execute(sql)\n",
    "        results = cursor.fetchall()\n",
    "        data.append({\n",
    "                'id': row[0],\n",
    "                'title': row[1],\n",
    "                'content': row[2],\n",
    "                'type': 'page'\n",
    "            })\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16151\n"
     ]
    }
   ],
   "source": [
    "data = get_data_from_table('douban')\n",
    "data += get_data_from_table('html')\n",
    "data += get_data_from_table('page')\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对网页内容进行聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "# from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut word finished\n",
      "tf-idf finished\n",
      "(16151, 49947)\n",
      "kmeans finished\n",
      "Cluster 0 has 5425 data points\n",
      "Cluster 1 has 951 data points\n",
      "Cluster 2 has 1927 data points\n",
      "Cluster 3 has 111 data points\n",
      "Cluster 4 has 340 data points\n",
      "Cluster 5 has 280 data points\n",
      "Cluster 6 has 221 data points\n",
      "Cluster 7 has 129 data points\n",
      "Cluster 8 has 975 data points\n",
      "Cluster 9 has 170 data points\n",
      "Cluster 10 has 78 data points\n",
      "Cluster 11 has 297 data points\n",
      "Cluster 12 has 319 data points\n",
      "Cluster 13 has 615 data points\n",
      "Cluster 14 has 2513 data points\n",
      "Cluster 15 has 201 data points\n",
      "Cluster 16 has 112 data points\n",
      "Cluster 17 has 144 data points\n",
      "Cluster 18 has 44 data points\n",
      "Cluster 19 has 367 data points\n",
      "Cluster 20 has 200 data points\n",
      "Cluster 21 has 195 data points\n",
      "Cluster 22 has 355 data points\n",
      "Cluster 23 has 182 data points\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def random_substring(s, length=50):\n",
    "    if len(s) <= length:\n",
    "        return s\n",
    "    start = random.randint(0, len(s) - length)\n",
    "    return s[start:start+length]\n",
    "\n",
    "with open('cn_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stop_words = [line.strip() for line in f.readlines()]\n",
    "stop_words += [\"的\", \"了\", \"在\", \"是\", \"我\", \"有\", \"和\", \"就\", \"不\", \"人\", \"他\", \"这\", \"中\", \"大\", \"以\", \"到\", \"说\", \"等\", \"能\", \"也\", \"上\", \"或\", \"之\", \"但\", \"个\", \"都\", \"而\", \"啊\", \"把\", \"那\", \"你\", \"一\", \"为\", \"所\", \"年\", \"没\", \"着\", \"要\", \"与\"]\n",
    "stop_words = list(set(stop_words))\n",
    "\n",
    "# 使用jieba进行分词\n",
    "contents = [' '.join(word for word in jieba.cut(item['title']+random_substring(item['content'])) \n",
    "                     if word not in stop_words) for item in data]\n",
    "print('cut word finished')\n",
    "\n",
    "# 使用TF-IDF模型将文本转换为数值向量\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(contents)\n",
    "print('tf-idf finished')\n",
    "print(X.shape)\n",
    "\n",
    "# # 使用PCA进行降维\n",
    "# pca = PCA(n_components=10000)\n",
    "# X_pca = pca.fit_transform(X.toarray())  # 转换为数组并降维\n",
    "# print('PCA finished')\n",
    "\n",
    "# 使用KMeans进行聚类\n",
    "kmeans = KMeans(n_clusters=24)\n",
    "kmeans.fit(X)\n",
    "print('kmeans finished')\n",
    "\n",
    "# 输出每个数据点的聚类标签\n",
    "labels = kmeans.labels_\n",
    "# 统计每个类的个数\n",
    "counts = np.bincount(labels)\n",
    "for i, count in enumerate(counts):\n",
    "    print(f\"Cluster {i} has {count} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存聚类结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS cluster\n",
    "\"\"\")\n",
    "cnx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建表\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS cluster (\n",
    "        id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        title VARCHAR(255),\n",
    "        type VARCHAR(255),\n",
    "        web_id INT,\n",
    "        label INT\n",
    "    )\n",
    "\"\"\")\n",
    "# labels是KMeans的结果，vectors是TF-IDF向量\n",
    "for item, label in zip(data, labels):\n",
    "    # 插入数据\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO cluster (title, type, web_id, label)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "    \"\"\", (item['title'], item['type'], item['id'], int(label)))\n",
    "\n",
    "# 提交\n",
    "cnx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_model.pkl']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# 保存模型\n",
    "joblib.dump(kmeans, 'kmeans_model.pkl')\n",
    "joblib.dump(vectorizer, 'tfidf_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取模型进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "kmeans = joblib.load('kmeans_model.pkl')\n",
    "vectorizer = joblib.load('tfidf_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query is in cluster 0\n"
     ]
    }
   ],
   "source": [
    "query = '霸王别姬'\n",
    "# 使用jieba进行分词\n",
    "query_cut = ' '.join(jieba.cut(query))\n",
    "# 使用TF-IDF模型将文本转换为数值向量\n",
    "query_vec = vectorizer.transform([query_cut])\n",
    "# 使用KMeans进行预测\n",
    "query_label = kmeans.predict(query_vec)\n",
    "print(f\"The query is in cluster {query_label[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取对应类别下所有记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5425\n"
     ]
    }
   ],
   "source": [
    "# 查询数据库\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT web_id, title\n",
    "    FROM cluster\n",
    "    WHERE label = %s\n",
    "\"\"\", (query_label[0],))\n",
    "\n",
    "# 获取查询结果\n",
    "results = cursor.fetchall()\n",
    "\n",
    "print(len(results))\n",
    "\n",
    "# # 打印结果\n",
    "# for row in results:\n",
    "#     print(f\"web_id: {row[0]}, title: {row[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "web_id: 1372, title: 迷恋与背叛——[霸王别姬], similarity: 0.5751382418948486\n",
      "web_id: 1373, title: 胡说霸王别姬, similarity: 0.6384269871485116\n",
      "web_id: 1376, title: 从另一角度看《霸王别姬》, similarity: 0.698602876133242\n",
      "web_id: 1378, title: 张国荣评《霸王别姬》, similarity: 0.710826486655719\n",
      "web_id: 4447, title: 霸王别姬, similarity: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import jieba.analyse\n",
    "\n",
    "# 获取所有的title\n",
    "titles = [row[1] for row in results]\n",
    "\n",
    "# 使用jieba进行分词\n",
    "titles = [' '.join(jieba.cut(title)) for title in titles]\n",
    "\n",
    "# 使用TF-IDF模型将query和titles转换为向量\n",
    "X = vectorizer.transform([query_cut] + titles)\n",
    "\n",
    "# 计算query和每个title的余弦相似度\n",
    "similarities = cosine_similarity(X[0:1], X[1:]).flatten()\n",
    "\n",
    "# 获取最相似的5个title的索引\n",
    "top5_indices = similarities.argsort()[-5:]\n",
    "\n",
    "# 打印最相似的5个title\n",
    "for index in top5_indices:\n",
    "    title = results[index][1]\n",
    "    print(f\"web_id: {results[index][0]}, title: {title}, similarity: {similarities[index]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
