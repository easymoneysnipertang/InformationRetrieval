{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 豆瓣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import random\n",
    "\n",
    "user_agents = [\n",
    "    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "    \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n",
    "    \"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n",
    "    \"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\",\n",
    "    \"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\",\n",
    "    \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\",\n",
    "]\n",
    "\n",
    "# user-agent\n",
    "request_header = {\n",
    "    \"User-Agent\":\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 \"\n",
    "        \"Safari/537.36 Edg/114.0.1823.58\",\n",
    "    \"Connection\":\n",
    "        \"keep-alive\",\n",
    "    \"Referer\":\n",
    "        \"https://www.douban.com\"\n",
    "}\n",
    "url_pool = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mysql数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "# 创建连接\n",
    "cnx = pymysql.connect(host='localhost', user='root', password='123qwe12')\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "def create_database():\n",
    "    # 创建数据库\n",
    "    cursor.execute(\"CREATE DATABASE IF NOT EXISTS IR_db\")\n",
    "    # 使用数据库\n",
    "    cnx.select_db('IR_db')\n",
    "\n",
    "def create_table():\n",
    "    # 创建表\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS douban (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            title VARCHAR(255),\n",
    "            content TEXT,\n",
    "            url VARCHAR(255),\n",
    "            links TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "    # 提交\n",
    "    cnx.commit()\n",
    "\n",
    "create_database()\n",
    "create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate(url):\n",
    "    # 查询数据\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT * FROM douban WHERE url = %s\n",
    "    \"\"\", (url,))\n",
    "    if cursor.fetchone() is None:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def save_data(title,content,url,links):\n",
    "    if check_duplicate(url):\n",
    "        return\n",
    "    # 插入数据\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO douban (title, content, url, links) VALUES (%s, %s, %s, %s)\n",
    "    \"\"\", (title, content, url, links))\n",
    "    # 提交\n",
    "    cnx.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理网页外链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_review_content(url):\n",
    "    resp = requests.get(url, headers=request_header)\n",
    "    status_code = resp.status_code\n",
    "    if status_code == 200:\n",
    "        html_text = resp.text\n",
    "        # 解析html\n",
    "        soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "        article = soup.select('#content > div > div.article')[0]\n",
    "        title = article.select('h1 > span')[0].text\n",
    "        content_div = article.select('div.main-bd > div > div.review-content.clearfix')[0]\n",
    "        content = content_div.text\n",
    "        a = soup.select('#content > div > div.aside > div > div.subject-title > a')[0]\n",
    "        link = a.get('href')\n",
    "        # 去掉链接中的查询参数\n",
    "        link = link.split('?')[0]\n",
    "        save_data(title, content, url, link)                \n",
    "    else:\n",
    "        print('请求失败')\n",
    "        \n",
    "\n",
    "def deal_review(detail_soup):\n",
    "    review_items = detail_soup.select('div.review-list > div > div > div.main-bd')\n",
    "    links = []\n",
    "    for item in review_items:\n",
    "        a = item.select('h2 > a')[0]\n",
    "        url = a.get('href')\n",
    "        # 去掉链接中的查询参数\n",
    "        url = url.split('?')[0]\n",
    "        links.append(url)\n",
    "        # 直接保存影评内容\n",
    "        deal_review_content(url)\n",
    "    return links\n",
    "\n",
    "\n",
    "def deal_recommend(detail_soup):\n",
    "    dls = detail_soup.select('#db-rec-section > div > dl')\n",
    "    links = []\n",
    "    for dl in dls:\n",
    "        a = dl.select('dd > a')[0]\n",
    "        url = a.get('href')\n",
    "        # 去掉链接中的查询参数\n",
    "        url = url.split('?')[0]\n",
    "        # 推荐影片放入url池\n",
    "        if url not in url_pool:\n",
    "            url_pool.append(url)\n",
    "        links.append(url)\n",
    "    return links\n",
    "\n",
    "def deal_recommend_book(detail_soup):\n",
    "    dls = detail_soup.select('#recommendations > div > dl')\n",
    "    links = []\n",
    "    for dl in dls:\n",
    "        a = dl.select('dd > a')[0]\n",
    "        url = a.get('href')\n",
    "        # 去掉链接中的查询参数\n",
    "        url = url.split('?')[0]\n",
    "        # 推荐影片放入url池\n",
    "        if url not in url_pool:\n",
    "            url_pool.append(url)\n",
    "        links.append(url)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理电影详情页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def deal_hd(hd_div):\n",
    "    homepage_a = hd_div.select('div.hd > a')[0]\n",
    "    homepage_url = homepage_a.get('href').strip()\n",
    "    if check_duplicate(homepage_url) is False:\n",
    "        url_pool.append(homepage_url)\n",
    "        while len(url_pool) > 0:\n",
    "            deal_url()\n",
    "    else:\n",
    "        print(f\"已经处理过url：{homepage_url}\") \n",
    "\n",
    "\n",
    "def deal_url_book():\n",
    "    homepage_url = url_pool.pop(0)\n",
    "    try:\n",
    "        request_header['User-Agent'] = random.choice(user_agents)\n",
    "        response = requests.get(url=homepage_url, headers=request_header)\n",
    "        status = response.status_code\n",
    "    except:\n",
    "        print(f\"请求失败{homepage_url}\")\n",
    "        request_header['User-Agent'] = random.choice(user_agents)\n",
    "        return\n",
    "    \n",
    "    if status == 200:\n",
    "        try:\n",
    "            # 进入详情页\n",
    "            detail_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            # 获取标题\n",
    "            title = detail_soup.select('#wrapper > h1 > span')[0].getText().strip()\n",
    "            # 获取网页内容 content = info+intro+author+comment\n",
    "            info = detail_soup.select('#info')[0].getText().strip()\n",
    "            intro_div = detail_soup.select('div#link-report div.intro')[0]\n",
    "            intro = [' '.join(p.text.split()) for p in intro_div.select('p')]\n",
    "            intro = \"\\n\".join(intro)\n",
    "            author_div = detail_soup.select('div.indent div.intro')[len(detail_soup.select('div.indent div.intro'))-1]\n",
    "            author = [' '.join(p.text.split()) for p in author_div.select('p')]\n",
    "            author = \"\\n\".join(author)\n",
    "            comment_lis = detail_soup.select('div#score > div > ul > li.comment-item')\n",
    "            comments = []\n",
    "            for comment_li in comment_lis:\n",
    "                comment = comment_li.select('div.comment > p > span')[0].getText().strip()\n",
    "                comments.append(comment)\n",
    "            comments = \"\\n\".join(comments)\n",
    "            content = \"\\n\".join([info, intro, author, comments])\n",
    "        except:\n",
    "            print(f\"解析失败{homepage_url}\")\n",
    "            return\n",
    "        try:\n",
    "            # 处理网页外链\n",
    "            links_review = deal_review(detail_soup)\n",
    "            links_recommend = deal_recommend_book(detail_soup)\n",
    "            links = \"\\n\".join(links_review + links_recommend)\n",
    "        except:\n",
    "            print(f\"解析外链失败{homepage_url}\")\n",
    "            return\n",
    "        try:\n",
    "            # 保存数据\n",
    "            save_data(title, content, homepage_url, links)\n",
    "        except:\n",
    "            print(f\"保存失败{homepage_url}\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"状态码非200\")\n",
    "\n",
    "\n",
    "def deal_url():\n",
    "    homepage_url = url_pool.pop(0)\n",
    "    try:\n",
    "        request_header['User-Agent'] = random.choice(user_agents)\n",
    "        response = requests.get(url=homepage_url, headers=request_header)\n",
    "        status = response.status_code\n",
    "    except:\n",
    "        print(f\"请求失败{homepage_url}\")\n",
    "        request_header['User-Agent'] = random.choice(user_agents)\n",
    "        return\n",
    "    \n",
    "    if status == 200:\n",
    "        try:\n",
    "            # 进入详情页\n",
    "            detail_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            # 获取标题\n",
    "            title = detail_soup.select('#content > h1 > span')[0].getText().strip()\n",
    "            # 获取网页内容 content = info+summary+comment\n",
    "            info = detail_soup.select('#info')[0].getText().strip()\n",
    "            summary_spans = detail_soup.select('div#link-report-intra span[property=\"v:summary\"]')\n",
    "            summary_t = [' '.join(span.getText().split()) for span in summary_spans]\n",
    "            summary = \"\\n\".join(summary_t)\n",
    "            comment_divs = detail_soup.select('#hot-comments > div.comment-item')\n",
    "            comments = []\n",
    "            for comment_div in comment_divs:\n",
    "                comment = comment_div.select('div.comment > p > span')[0].getText().strip()\n",
    "                comments.append(comment)\n",
    "            comments = \"\\n\".join(comments)\n",
    "            content = \"\\n\".join([info, summary, comments])\n",
    "        except:\n",
    "            print(f\"解析失败{homepage_url}\")\n",
    "            return\n",
    "        try:\n",
    "            # 处理网页外链\n",
    "            links_review = deal_review(detail_soup)\n",
    "            links_recommend = deal_recommend(detail_soup)\n",
    "            links = \"\\n\".join(links_review + links_recommend)\n",
    "        except:\n",
    "            print(f\"解析外链失败{homepage_url}\")\n",
    "            return\n",
    "        try:\n",
    "            # 保存数据\n",
    "            save_data(title, content, homepage_url, links)\n",
    "        except:\n",
    "            print(f\"保存失败{homepage_url}\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"状态码非200\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进入top250主页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析top250主页，what = movie/book\n",
    "def search_in_top250(what):\n",
    "    for i in range(0,250,25):\n",
    "        # 生成url\n",
    "        url = \"https://{}.douban.com/top250?start={}\".format(what,i)\n",
    "        # 发送请求\n",
    "        resp = requests.get(url,headers=request_header)\n",
    "        status_code = resp.status_code\n",
    "        if status_code == 200:\n",
    "            html_text = resp.text\n",
    "            # 解析html\n",
    "            soup = BeautifulSoup(html_text,\"lxml\")\n",
    "            if what == 'movie':\n",
    "                # 获取到这25个电影的链接（存在div.hd里）\n",
    "                hd_divs = soup.select('div.item > div.info > div.hd')\n",
    "                for hd_div in hd_divs:\n",
    "                    deal_hd(hd_div)\n",
    "            else:\n",
    "                # 获取到这25个书籍的链接（存在div.pl2里）\n",
    "                pl2_divs = soup.select('tr.item > td > div.pl2')\n",
    "                for pl2_div in pl2_divs:\n",
    "                    a = pl2_div.select('a')[0]\n",
    "                    url = a.get('href').strip()\n",
    "                    if check_duplicate(url) is False:\n",
    "                        url_pool.append(url)\n",
    "                        while len(url_pool) > 0:\n",
    "                            deal_url_book()\n",
    "                    else:\n",
    "                        print(f\"已经处理过url：{url}\")\n",
    "        else:\n",
    "            print(\"Error in top250: {}\".format(status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_in_top250('book')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
